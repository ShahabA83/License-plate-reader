{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f7fa3b-d02b-49c7-b3a7-fd68e21ebd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys  \n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "import torch\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
    "\n",
    "sys.path.append(os.path.abspath('C:/Users/Shahab/Downloads/Jupyter/Car-license-Plate/My-Code/Project/yolov7'))\n",
    "from models.experimental import attempt_load\n",
    "from utils.general import check_img_size\n",
    "from utils.torch_utils import select_device, TracedModel\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression, scale_coords\n",
    "from utils.plots import plot_one_box, plot_one_box_PIL\n",
    "\n",
    "from copy import deepcopy\n",
    "import easyocr\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt, atan, degrees\n",
    "from tensorflow.keras.models import load_model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e921bc7-be82-4083-a6bb-9cd8a46c451d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "IDetect.fuse\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shahab\\qenv\\lib\\site-packages\\torch\\functional.py:539: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3638.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Convert model to Traced-model... \n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "weights = 'C:/Users/Shahab/Downloads/Jupyter/Car-license-Plate/My-Code/Project/yolov7/weights/best.pt'\n",
    "device_id = 'cpu'   #'cuda'\n",
    "image_size = 640\n",
    "trace = True\n",
    "\n",
    "# بارگذاری مدل  \n",
    "model_number = load_model('C:/Users/Shahab/Downloads/Jupyter/Car-license-Plate/My-Code/Project/model/model_number.h5') \n",
    "model_alphabet = load_model('C:/Users/Shahab/Downloads/Jupyter/Car-license-Plate/My-Code/Project/model/model_alphabet.h5')  \n",
    "\n",
    "# Initialize\n",
    "device = select_device(device_id)\n",
    "half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "\n",
    "# Load model\n",
    "model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "stride = int(model.stride.max())  # model stride\n",
    "imgsz = check_img_size(image_size, s=stride)  # check img_size\n",
    "\n",
    "#The model is tracked for optimization.\n",
    "if trace:\n",
    "    model = TracedModel(model, device, image_size)\n",
    "\n",
    "# Only supported on CUDA\n",
    "# Faster calculations\n",
    "if half:\n",
    "    model.half()  # to FP16\n",
    "\n",
    "# If a GPU is used, the model is run once to perform an initial warm-up operation.\n",
    "if device.type != 'cpu':\n",
    "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "\n",
    "# Load OCR\n",
    "#reader = easyocr.Reader(['fa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc99f72f-40b0-4eaa-94d0-6c47bbfa8a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_plate(source_image):\n",
    "    # Padded resize\n",
    "    img_size = 640\n",
    "    stride = 32\n",
    "    img = letterbox(source_image, img_size, stride=stride)[0]\n",
    "\n",
    "    # Convert\n",
    "    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "    img = np.ascontiguousarray(img)\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        # Inference\n",
    "        pred = model(img, augment=True)[0]\n",
    "\n",
    "    # Apply NMS\n",
    "    pred = non_max_suppression(pred, 0.25, 0.45, classes=0, agnostic=True)\n",
    "\n",
    "    plate_detections = []\n",
    "    det_confidences = []\n",
    "    \n",
    "    # Process detections\n",
    "    for i, det in enumerate(pred):  # detections per image\n",
    "        if len(det):\n",
    "            # Rescale boxes from img_size to im0 size\n",
    "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], source_image.shape).round()\n",
    "\n",
    "            # Return results\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                coords = [int(position) for position in (torch.tensor(xyxy).view(1, 4)).tolist()[0]]\n",
    "                plate_detections.append(coords)\n",
    "                det_confidences.append(conf.item())\n",
    "\n",
    "    return plate_detections, det_confidences\n",
    "\n",
    "\"\"\"\n",
    "def unsharp_mask(image, kernel_size=(5, 5), sigma=1.0, amount=2.0, threshold=0):\n",
    "    blurred = cv.GaussianBlur(image, kernel_size, sigma)\n",
    "    sharpened = float(amount + 1) * image - float(amount) * blurred\n",
    "    sharpened = np.maximum(sharpened, np.zeros(sharpened.shape))\n",
    "    sharpened = np.minimum(sharpened, 255 * np.ones(sharpened.shape))\n",
    "    sharpened = sharpened.round().astype(np.uint8)\n",
    "    if threshold > 0:\n",
    "        low_contrast_mask = np.absolute(image - blurred) < threshold\n",
    "        np.copyto(sharpened, image, where=low_contrast_mask)\n",
    "    return sharpened\n",
    "\"\"\"\n",
    "\n",
    "def crop(image, coord):\n",
    "    cropped_image = image[int(coord[1]):int(coord[3]), int(coord[0]):int(coord[2])]\n",
    "    return cropped_image\n",
    "\n",
    "\"\"\"\n",
    "counter_ocr_plate = 0\n",
    "def ocr_plate(plate_region):\n",
    "    global counter_ocr_plate\n",
    "    # Image pre-processing for more accurate OCR\n",
    "    #cv.imwrite(os.path.join(savepath, \"plate_img.png\"), plate_region)\n",
    "    rescaled = cv.resize(plate_region, None, fx=1.2, fy=1.2, interpolation=cv.INTER_CUBIC)\n",
    "    grayscale = cv.cvtColor(rescaled, cv.COLOR_BGR2GRAY)\n",
    "    # OCR the preprocessed image\n",
    "    grayscale_blur = cv.medianBlur(grayscale, 1)\n",
    "    ret, thresh1 = cv.threshold(grayscale_blur, 120, 255, cv.THRESH_BINARY + cv.THRESH_OTSU) \n",
    "    cv.imwrite(os.path.join(savepath, \"grayscale_blur%s.png\" %counter_ocr_plate), grayscale_blur)\n",
    "    counter_ocr_plate += 1\n",
    "    plate_text_easyocr = reader.readtext(grayscale_blur)\n",
    "    if plate_text_easyocr:\n",
    "        (bbox, text_easyocr, ocr_confidence) = plate_text_easyocr[0]\n",
    "        #print(\"plate_text Easyocr \", text_easyocr)\n",
    "    else:\n",
    "        text_easyocr = \"_\"\n",
    "        ocr_confidence = 0\n",
    "    #if ocr_confidence == 'nan':\n",
    "    \n",
    "    return text_easyocr, ocr_confidence\n",
    "\"\"\"\n",
    "\n",
    "#Rotate Pelak\n",
    "def find_longest_line(plate_img_gr):\n",
    "    kernel_size = 3\n",
    "    blur_gray = cv.GaussianBlur(plate_img_gr, (kernel_size, kernel_size), 0)\n",
    "\n",
    "    low_threshold = 150\n",
    "    high_threshold = 200\n",
    "\n",
    "    edges = cv.Canny(blur_gray, low_threshold, high_threshold)\n",
    "\n",
    "    rho = 1  # distance resolution in pixels of the Hough grid\n",
    "    theta = np.pi / 180  # angular resolution in radians of the Hough grid\n",
    "    threshold = 15  # minimum number of votes (intersections in Hough grid cell)\n",
    "    min_line_length = 50  # minimum number of pixels making up a line\n",
    "    max_line_gap = 5  # maximum gap in pixels between connectable line segments\n",
    "    line_image = np.copy(plate_img_gr) * 0  # creating a blank to draw lines on\n",
    "\n",
    "    # Run Hough on edge detected image\n",
    "    # Output \"lines\" is an array containing endpoints of detected line segments\n",
    "    lines = cv.HoughLinesP(edges, rho, theta, threshold, np.array([]),\n",
    "                        min_line_length, max_line_gap)\n",
    "\n",
    "    \n",
    "    lls = []\n",
    "    for indx, line in enumerate(lines):\n",
    "        for x1,y1,x2,y2 in line:\n",
    "            cv.line(line_image,(x1,y1),(x2,y2),(255,0,0),5)\n",
    "            line_length = sqrt((x2-x1)**2 + (y2-y1)**2)\n",
    "            lls.append((indx,line_length))\n",
    "    lls.sort(key = lambda x: x[1])\n",
    "    linessorted = []\n",
    "    for (indx,ll) in lls:\n",
    "        linessorted.append(lines[indx])\n",
    "    return linessorted\n",
    "\n",
    "def find_line_angle(line):\n",
    "    x1,y1,x2,y2 = line[0]\n",
    "    angle = degrees(atan(((y2-y1)/(x2-x1))))\n",
    "    return angle\n",
    "\n",
    "def rotate_image(plate_img_gr, angle):\n",
    "    (h, w) = plate_img_gr.shape\n",
    "    (cX, cY) = (w // 2, h // 2)\n",
    "    M = cv.getRotationMatrix2D((cX, cY), angle, 1.0)\n",
    "    rotated = cv.warpAffine(plate_img_gr, M, (w, h))\n",
    "    return rotated\n",
    "\n",
    "def adjust_cropping(rotated_img):\n",
    "    h,w = rotated_img.shape\n",
    "    targ_h = int(w/4)\n",
    "    crop_h = int((h - targ_h)/2)\n",
    "    cropped_rotated_img = rotated_img[crop_h:h-crop_h,:]\n",
    "    return cropped_rotated_img\n",
    "\n",
    "#تیکه تیکه کردن پلاک\n",
    "\n",
    "\n",
    "def Cutting_Pieces (plate_region,pcc):\n",
    "    h, w = plate_region.shape\n",
    "    chopfactors = [(60,125), (125,190), (180,300), (280,350), (340,415), (400,465), (460,535), (515,600)]\n",
    "    plate_letters = []\n",
    "    c = 1\n",
    "    for factor in chopfactors:\n",
    "        w1 = int((factor[0]/600)*w)\n",
    "        w2 = int((factor[1]/600)*w)\n",
    "        letterpatch = plate_region[:,w1:w2]\n",
    "        letterpatch_resized  = cv.resize(letterpatch, (28,28), interpolation = cv.INTER_LINEAR)\n",
    "        #ax[c].imshow(letterpatch_resized, cmap='gray')\n",
    "        cv.imwrite(os.path.join(savepath, \"pieces/%s-%s.png\" %(pcc,c)), letterpatch_resized)\n",
    "        c += 1\n",
    "        plate_letters.append(letterpatch_resized)\n",
    "    return plate_letters\n",
    "\n",
    "cont = 5100\n",
    "#Model Number and alphabet\n",
    "def pred_number_alphabet(list_img):\n",
    "    global model_alphabet\n",
    "    global model_number\n",
    "    global cont\n",
    "    lst = []\n",
    "    c = 0\n",
    "    for i in list_img:\n",
    "        img = i.copy()\n",
    "        if c != 2: \n",
    "            i = i / 255\n",
    "            i = np.array([i])\n",
    "            y_pred = model_number.predict(i)\n",
    "            result = y_pred.argmax()\n",
    "            lst.append(str(result))\n",
    "        else:\n",
    "            i = i / 255\n",
    "            i = np.array([i])\n",
    "            y_pred = model_alphabet.predict(i)\n",
    "            result = y_pred.argmax()\n",
    "            if result == 10:\n",
    "                lst.append('B')\n",
    "            elif result == 11:\n",
    "                lst.append('Q')\n",
    "            elif result == 12:\n",
    "                lst.append('H')\n",
    "            elif result == 13:\n",
    "                lst.append('SAD')\n",
    "            elif result == 14:\n",
    "                lst.append('TA')\n",
    "            elif result == 15:\n",
    "                lst.append('V')\n",
    "            elif result == 16:\n",
    "                lst.append('M')\n",
    "            elif result == 17:\n",
    "                lst.append('J')\n",
    "            elif result == 18:\n",
    "                lst.append('Y')\n",
    "            elif result == 19:\n",
    "                lst.append('EIN')\n",
    "            elif result == 20:\n",
    "                lst.append('N')\n",
    "            elif result == 21:\n",
    "                lst.append('D')\n",
    "            elif result == 22:\n",
    "                lst.append('T')\n",
    "            elif result == 23:\n",
    "                lst.append('HE')\n",
    "            elif result == 24:\n",
    "                lst.append('L')\n",
    "            elif result == 25:\n",
    "                lst.append('SIN')\n",
    "        cv.imwrite(os.path.join(savepath, \"pred/%s-%s.png\" %(cont,result)), img)\n",
    "        cont += 1\n",
    "            \n",
    "        c += 1\n",
    "    print(lst)\n",
    "    return lst\n",
    "\n",
    "\n",
    "ro = 1\n",
    "pc = 1\n",
    "# Main function\n",
    "def get_plates_from_image(input):\n",
    "    if input is None:\n",
    "        return None\n",
    "    global ro\n",
    "    global pc\n",
    "    plate_detections, det_confidences = detect_plate(input)\n",
    "    detected_image = deepcopy(input)\n",
    "    \n",
    "    for coords in plate_detections:\n",
    "        try:\n",
    "            plate_region = crop(input, coords)\n",
    "            #Rotate Pelak\n",
    "            plate_img = plate_region.copy()\n",
    "            plate_img_gr = cv.cvtColor(plate_region, cv.COLOR_BGR2GRAY)\n",
    "            \n",
    "            try:\n",
    "                linessorted = find_longest_line(plate_img_gr)\n",
    "                rot_angle = find_line_angle(linessorted[-1])\n",
    "                rotated_img = rotate_image(plate_img_gr, rot_angle)\n",
    "                cropped_rotated_img = adjust_cropping(rotated_img)\n",
    "                w, h = cropped_rotated_img.shape\n",
    "                if w < 10 or h < 10:\n",
    "                    cv.imwrite(os.path.join(savepath, \"license-plate/%s.png\" %ro), plate_img_gr)\n",
    "                    ro += 1\n",
    "                    list_plate_letter = Cutting_Pieces(plate_img_gr, pc)\n",
    "                else :\n",
    "                    cv.imwrite(os.path.join(savepath, \"license-plate/%s.png\" %ro), cropped_rotated_img)\n",
    "                    ro += 1\n",
    "                    #تیکه تیکه کردن عکس \n",
    "                    list_plate_letter = Cutting_Pieces(cropped_rotated_img, pc)\n",
    "                \n",
    "            except:\n",
    "                #تیکه تیکه کردن عکس\n",
    "                cv.imwrite(os.path.join(savepath, \"license-plate/%s.png\" %ro), plate_img_gr)\n",
    "                ro += 1\n",
    "                list_plate_letter = Cutting_Pieces(plate_img_gr, pc)\n",
    "            pc += 1\n",
    "            #پیشبینی اعداد\n",
    "            y_preds = pred_number_alphabet(list_plate_letter)\n",
    "            str_y_preds = ''.join(y_preds)\n",
    "            detected_image = plot_one_box_PIL(coords, detected_image, label=str_y_preds, color=[0, 150, 255], line_thickness=2)\n",
    "        \n",
    "        except:\n",
    "            print('Error')\n",
    "            continue\n",
    "            \n",
    "    return detected_image\n",
    "\n",
    "\"\"\"\n",
    "def pascal_voc_to_coco(x1y1x2y2):\n",
    "    x1, y1, x2, y2 = x1y1x2y2\n",
    "    return [x1, y1, x2 - x1, y2 - y1]\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def get_best_ocr(preds, rec_conf, ocr_res, track_id):\n",
    "    for info in preds:\n",
    "    # Check if it is current track id\n",
    "        if info['track_id'] == track_id:\n",
    "          # Check if the ocr confidenence is maximum or not\n",
    "            if info['ocr_conf'] < rec_conf:\n",
    "                info['ocr_conf'] = rec_conf\n",
    "                info['ocr_txt'] = ocr_res\n",
    "            else:\n",
    "                rec_conf = info['ocr_conf']\n",
    "                ocr_res = info['ocr_txt']\n",
    "            break\n",
    "    return preds, rec_conf, ocr_res\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def get_plates_from_video(source):\n",
    "    if source is None:\n",
    "        return None\n",
    "    \n",
    "    # Create a VideoCapture object\n",
    "    video = cv.VideoCapture(source)\n",
    "\n",
    "    # Default resolutions of the frame are obtained. The default resolutions are system dependent.\n",
    "    # We convert the resolutions from float to integer.\n",
    "    width = int(video.get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = video.get(cv.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object.\n",
    "    temp = f'{Path(source).stem}_temp{Path(source).suffix}'\n",
    "    export = cv.VideoWriter(temp, cv.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "    \n",
    "    # Intializing tracker\n",
    "    tracker = DeepSort(embedder_gpu=False)\n",
    "    \n",
    "    # Initializing some helper variables.\n",
    "    preds = []\n",
    "    total_obj = 0\n",
    "\n",
    "    while(True):\n",
    "        ret, frame = video.read()\n",
    "        if ret == True:\n",
    "            # Run the ANPR algorithm\n",
    "            bboxes, scores = detect_plate(frame)\n",
    "            # Convert Pascal VOC detections to COCO\n",
    "            bboxes = list(map(lambda bbox: pascal_voc_to_coco(bbox), bboxes))\n",
    "            \n",
    "            if len(bboxes) > 0:\n",
    "                # Storing all the required info in a list.\n",
    "                detections = [(bbox, score, 'number_plate') for bbox, score in zip(bboxes, scores)]\n",
    "\n",
    "                # Applying tracker.\n",
    "                # The tracker code flow: kalman filter -> target association(using hungarian algorithm) and appearance descriptor.\n",
    "                tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "                # Checking if tracks exist.\n",
    "                for track in tracks:\n",
    "                    if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                        continue\n",
    "\n",
    "                    # Changing track bbox to top left, bottom right coordinates\n",
    "                    bbox = [int(position) for position in list(track.to_tlbr())]\n",
    "                    \n",
    "                    for i in range(len(bbox)):\n",
    "                        if bbox[i] < 0:\n",
    "                            bbox[i] = 0\n",
    "\n",
    "                    # Cropping the license plate and applying the OCR.\n",
    "                    plate_region = crop(frame, bbox)\n",
    "                    plate_text, ocr_confidence = ocr_plate(plate_region)\n",
    "\n",
    "                    # Storing the ocr output for corresponding track id.\n",
    "                    output_frame = {'track_id': track.track_id, 'ocr_txt': plate_text, 'ocr_conf': ocr_confidence}\n",
    "\n",
    "                    # Appending track_id to list only if it does not exist in the list\n",
    "                    # else looking for the current track in the list and updating the highest confidence of it.\n",
    "                    if track.track_id not in list(set(pred['track_id'] for pred in preds)):\n",
    "                        total_obj += 1\n",
    "                        preds.append(output_frame)\n",
    "                    else:\n",
    "                        preds, ocr_confidence, plate_text = get_best_ocr(preds, ocr_confidence, plate_text, track.track_id)\n",
    "                    \n",
    "                    # Plotting the prediction.\n",
    "                    frame = plot_one_box_PIL(bbox, frame, label=f'{str(track.track_id)}. {plate_text}', color=[255, 150, 0], line_thickness=3)\n",
    "                    cv.imshow(\"frame \", frame)\n",
    "                    keyexit = cv.waitKey(0)\n",
    "                    if keyexit == 27:\n",
    "                        break\n",
    "            # Write the frame into the output file\n",
    "            export.write(frame)\n",
    "        else:\n",
    "            break \n",
    "\n",
    "    # When everything done, release the video capture and video write objects\n",
    "    cv.destroyAllWindows()\n",
    "    video.release()\n",
    "    export.release()\n",
    "\n",
    "    # Compressing the output video for smaller size and web compatibility.\n",
    "    output = f'{Path(source).stem}_detected{Path(source).suffix}'\n",
    "    os.system(f'ffmpeg -y -i {temp} -c:v libx264 -b:v 5000k -minrate 1000k -maxrate 8000k -pass 1 -c:a aac -f mp4 /dev/null && ffmpeg -i {temp} -c:v libx264 -b:v 5000k -minrate 1000k -maxrate 8000k -pass 2 -c:a aac -movflags faststart {output}')\n",
    "    os.system(f'rm -rf {temp} ffmpeg2pass-0.log ffmpeg2pass-0.log.mbtree')\n",
    "\n",
    "    return output\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "def get_plates_from_webcam():\n",
    "    \n",
    "    # Create a VideoCapture object\n",
    "    video = cv.VideoCapture(0)\n",
    "\n",
    "    # Default resolutions of the frame are obtained. The default resolutions are system dependent.\n",
    "    # We convert the resolutions from float to integer.\n",
    "    width = int(video.get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = video.get(cv.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object.\n",
    "    temp = f'cam_temp.mp4'\n",
    "    export = cv.VideoWriter(temp, cv.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "    \n",
    "    # Intializing tracker\n",
    "    tracker = DeepSort(embedder_gpu=False)\n",
    "    \n",
    "    # Initializing some helper variables.\n",
    "    preds = []\n",
    "    total_obj = 0\n",
    "    fr_count = 0\n",
    "    while(True):\n",
    "        ret, frame = video.read()\n",
    "        if ret == True:\n",
    "            \n",
    "            fr_count+=1\n",
    "            if fr_count % 10 !=0:\n",
    "                continue\n",
    "\n",
    "            # Run the ANPR algorithm\n",
    "            bboxes, scores = detect_plate(frame)\n",
    "            # Convert Pascal VOC detections to COCO\n",
    "            bboxes = list(map(lambda bbox: pascal_voc_to_coco(bbox), bboxes))\n",
    "            \n",
    "            if len(bboxes) > 0:\n",
    "                # Storing all the required info in a list.\n",
    "                detections = [(bbox, score, 'number_plate') for bbox, score in zip(bboxes, scores)]\n",
    "\n",
    "                # Applying tracker.\n",
    "                # The tracker code flow: kalman filter -> target association(using hungarian algorithm) and appearance descriptor.\n",
    "                tracks = tracker.update_tracks(detections, frame=frame)\n",
    "\n",
    "                # Checking if tracks exist.\n",
    "                for track in tracks:\n",
    "                    if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                        continue\n",
    "\n",
    "                    # Changing track bbox to top left, bottom right coordinates\n",
    "                    bbox = [int(position) for position in list(track.to_tlbr())]\n",
    "                    \n",
    "                    for i in range(len(bbox)):\n",
    "                        if bbox[i] < 0:\n",
    "                            bbox[i] = 0\n",
    "\n",
    "                    # Cropping the license plate and applying the OCR.\n",
    "                    plate_region = crop(frame, bbox)\n",
    "                    plate_text, ocr_confidence = ocr_plate(plate_region)\n",
    "\n",
    "                    # Storing the ocr output for corresponding track id.\n",
    "                    output_frame = {'track_id': track.track_id, 'ocr_txt': plate_text, 'ocr_conf': ocr_confidence}\n",
    "\n",
    "                    # Appending track_id to list only if it does not exist in the list\n",
    "                    # else looking for the current track in the list and updating the highest confidence of it.\n",
    "                    if track.track_id not in list(set(pred['track_id'] for pred in preds)):\n",
    "                        total_obj += 1\n",
    "                        preds.append(output_frame)\n",
    "                    else:\n",
    "                        preds, ocr_confidence, plate_text = get_best_ocr(preds, ocr_confidence, plate_text, track.track_id)\n",
    "                    \n",
    "                    # Plotting the prediction.\n",
    "                    frame = plot_one_box_PIL(bbox, frame, label=f'{str(track.track_id)}. {plate_text}', color=[255, 150, 0], line_thickness=3)\n",
    "                    cv.imshow(\"frame \", frame)\n",
    "                    keyexit = cv.waitKey(0) \n",
    "                    if keyexit == 27:\n",
    "                        break\n",
    "            # Write the frame into the output file\n",
    "            export.write(frame)\n",
    "        else:\n",
    "            break \n",
    "\n",
    "    # When everything done, release the video capture and video write objects\n",
    "    cv.destroyAllWindows()\n",
    "    video.release()\n",
    "    export.release()\n",
    "\n",
    "    # Compressing the output video for smaller size and web compatibility.\n",
    "    output = f'cam_detected.mp4'\n",
    "    os.system(f'ffmpeg -y -i {temp} -c:v libx264 -b:v 5000k -minrate 1000k -maxrate 8000k -pass 1 -c:a aac -f mp4 /dev/null && ffmpeg -i {temp} -c:v libx264 -b:v 5000k -minrate 1000k -maxrate 8000k -pass 2 -c:a aac -movflags faststart {output}')\n",
    "    os.system(f'rm -rf {temp} ffmpeg2pass-0.log ffmpeg2pass-0.log.mbtree')\n",
    "\n",
    "    return output\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d320e74-605d-4c8a-9fd4-2190e00fcfea",
   "metadata": {},
   "source": [
    "# Running the model on a photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a161503c-53be-464f-8777-41162a139e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_n_vids_path = \"C:/Users/Shahab/Downloads/Jupyter/Car-license-Plate/My-Code/Project/Dataset/Car/car-picture\"\n",
    "image_path = os.path.join(images_n_vids_path, \"27.jpg\")\n",
    "#video_path = os.path.join(images_n_vids_path, \"test_video_short.mp4\")\n",
    "\n",
    "savepath = \"C:/Users/Shahab/Downloads/Jupyter/Car-license-Plate/My-Code/Project/Dataset/Final-model-outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8a78d7fd-60bf-42d0-9dae-879cda11bf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "['1', '7', 'TA', '2', '2', '2', '2', '8']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "['5', '8', 'Q', '2', '1', '8', '5', '7']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plate_image = cv.imread(image_path)\n",
    "detected_plate_image = get_plates_from_image(plate_image)\n",
    "cv.imwrite(os.path.join(savepath, \"detected_plate.png\"), detected_plate_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1cf13c-cf2b-48e1-a4de-3006fca0bab5",
   "metadata": {},
   "source": [
    "# Running the model on multiple images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd9f52dc-0556-48e3-9bbf-1d1c08317b90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "#savepath = \"C:/Users/Shahab/Downloads/Jupyter/Car-license-Plate/My-Code/Project/Dataset/Final-model-outputs2\"\n",
    "#imagefiles = glob.glob(\"C:/Users/Shahab/Downloads/Jupyter/Car-license-Plate/My-Code/Project/Dataset/Car/car-picture2/*.jpg\")\n",
    "\n",
    "savepath = \"C:/Users/Shahab/Downloads/Jupyter/Car-license-Plate/My-Code/Project/Dataset/Final-model-outputs\"\n",
    "imagefiles = glob.glob(\"C:/Users/Shahab/Downloads/Jupyter/Car-license-Plate/My-Code/Project/Dataset/Car/car-for-test-model/*.jpg\")\n",
    "imagefiles.sort()\n",
    "\n",
    "data = []\n",
    "for filename in imagefiles:\n",
    "    img = cv.imread(filename)\n",
    "    data.append(img)\n",
    "\n",
    "num_images = len(data)\n",
    "num_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d43ccd2-e40b-41ba-8b3f-bfaf3f55c13d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step\n",
      "['3', '4', 'SAD', '2', '9', '1', '2', '8']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "['5', '1', 'TA', '7', '6', '8', '2', '8']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "['6', '6', 'B', '7', '4', '8', '9', '9']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "['1', '2', 'D', '3', '4', '5', '6', '6']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "['9', '1', 'Q', '5', '2', '5', '1', '1']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
      "['1', '8', 'Q', '2', '6', '7', '4', '4']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "['6', '9', 'HE', '4', '3', '1', '5', '0']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "['7', '7', 'B', '6', '5', '7', '5', '8']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "['4', '4', 'J', '1', '4', '4', '4', '4']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "['1', '7', 'B', '6', '4', '1', '1', '0']\n"
     ]
    }
   ],
   "source": [
    "counter = 1\n",
    "for image in data:\n",
    "    #plate_image = cv.imread(image_path)\n",
    "    try:\n",
    "        detected_plate_image = get_plates_from_image(image)\n",
    "        cv.imwrite(os.path.join(savepath, \"detected_plate%s.png\" %counter), detected_plate_image)\n",
    "        counter += 1\n",
    "    except:\n",
    "        print('Error')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e365351-409d-496c-b686-77df21e4757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#detected_plate_video = get_plates_from_video(video_path)\n",
    "\n",
    "#detected_plate_webcam = get_plates_from_webcam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d0d573-71a2-4b38-ba95-93e361f903e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
